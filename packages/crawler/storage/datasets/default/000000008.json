{
	"title": "Function calling - OpenAI API",
	"url": "https://platform.openai.com/docs/guides/function-calling",
	"html": "Overview\nDocumentation\nAPI reference\nExamples\nLog in\nSign up‍\nSearch\n⌘\nK\nGET STARTED\nIntroduction\nQuickstart\nModels\nTutorials\nChangelog\nCAPABILITIES\nText generation\nFunction calling\nEmbeddings\nFine-tuning\nImage generation\nVision\nText-to-speech\nSpeech-to-text\nModeration\nASSISTANTS\nOverview\nHow Assistants work\nTools\nGUIDES\nPrompt engineering\nProduction best practices\nSafety best practices\nRate limits\nError codes\nLibraries\nDeprecations\nPolicies\nCHATGPT\nActions\nPlugins\nFunction calling\n\nLearn how to connect large language models to external tools.\n\nWe are aware of an issue with non-ASCII outputs in gpt-3.5-turbo-1106 and gpt-4-1106-preview, and are working on implementing a fix. When these models generate a function call and the arguments include non-ASCII characters, the API may return Unicode escape sequences instead of the Unicode character directly. For example, arguments may look like {\"location\": \"D\\u00fcsseldorf\"} instead of {\"location\": \"Düsseldorf\"}. Most applications should not be affected by this, as JSON parsers in languages like Python and Javascript will parse these strings into the correct objects. To stay updated on this topic, please subscribe to this community forum thread.\nIntroduction\n\nIn an API call, you can describe functions and have the model intelligently choose to output a JSON object containing arguments to call one or many functions. The Chat Completions API does not call the function; instead, the model generates JSON that you can use to call the function in your code.\n\nThe latest models (gpt-3.5-turbo-1106 and gpt-4-1106-preview) have been trained to both detect when a function should to be called (depending on the input) and to respond with JSON that adheres to the function signature more closely than previous models. With this capability also comes potential risks. We strongly recommend building in user confirmation flows before taking actions that impact the world on behalf of users (sending an email, posting something online, making a purchase, etc).\n\nThis guide is focused on function calling with the Chat Completions API, for details on function calling in the Assistants API, please see the Assistants Tools page.\nCommon use cases\n\nFunction calling allows you to more reliably get structured data back from the model. For example, you can:\n\nCreate assistants that answer questions by calling external APIs (e.g. like ChatGPT Plugins)\ne.g. define functions like send_email(to: string, body: string), or get_current_weather(location: string, unit: 'celsius' | 'fahrenheit')\nConvert natural language into API calls\ne.g. convert \"Who are my top customers?\" to get_customers(min_revenue: int, created_before: string, limit: int) and call your internal API\nExtract structured data from text\ne.g. define a function called extract_data(name: string, birthday: string), or sql_query(query: string)\n\n...and much more!\n\nThe basic sequence of steps for function calling is as follows:\n\nCall the model with the user query and a set of functions defined in the functions parameter.\nThe model can choose to call one or more functions; if so, the content will be a stringified JSON object adhering to your custom schema (note: the model may hallucinate parameters).\nParse the string into JSON in your code, and call your function with the provided arguments if they exist.\nCall the model again by appending the function response as a new message, and let the model summarize the results back to the user.\nSupported models\n\nNot all model versions are trained with function calling data. Function calling is supported with the following models:\n\ngpt-4\ngpt-4-1106-preview\ngpt-4-0613\ngpt-3.5-turbo\ngpt-3.5-turbo-1106\ngpt-3.5-turbo-0613\n\nIn addition, parallel function calls is supported on the following models:\n\ngpt-4-1106-preview\ngpt-3.5-turbo-1106\nParallel function calling\n\nParallel function calling is the model's ability to perform multiple function calls together, allowing the effects and results of these function calls to be resolved in parallel. This is especially useful if functions take a long time, and reduces round trips with the API. For example, the model may call functions to get the weather in 3 different locations at the same time, which will result in a message with 3 function calls in the tool_calls array, each with an id. To respond to these function calls, add 3 new messages to the conversation, each containing the result of one function call, with a tool_call_id referencing the id from tool_calls.\n\nIn this example, we define a single function get_current_weather. The model calls the function multiple times, and after sending the function response back to the model, we let it decide the next step. It responded with a user-facing message which was telling the user the temperature in San Francisco, Tokyo, and Paris. Depending on the query, it may choose to call a function again.\n\nIf you want to force the model to call a specific function you can do so by setting tool_choice with a specific function name. You can also force the model to generate a user-facing message by setting tool_choice: \"none\". Note that the default behavior (tool_choice: \"auto\") is for the model to decide on its own whether to call a function and if so which function to call.\n\nExample invoking multiple function calls in one response\n\nYou can find more examples of function calling in the OpenAI Cookbook:\nFunction calling\nLearn from more examples demonstrating function calling\nTokens\n\nUnder the hood, functions are injected into the system message in a syntax the model has been trained on. This means functions count against the model's context limit and are billed as input tokens. If running into context limits, we suggest limiting the number of functions or the length of documentation you provide for function parameters.\n\nIt is also possible to use fine-tuning to reduce the number of tokens used if you have many functions defined."
}