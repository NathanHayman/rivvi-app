{
	"title": "Text to speech - OpenAI API",
	"url": "https://platform.openai.com/docs/guides/text-to-speech",
	"html": "Overview\nDocumentation\nAPI reference\nExamples\nLog in\nSign up‍\nSearch\n⌘\nK\nGET STARTED\nIntroduction\nQuickstart\nModels\nTutorials\nChangelog\nCAPABILITIES\nText generation\nFunction calling\nEmbeddings\nFine-tuning\nImage generation\nVision\nText-to-speech\nSpeech-to-text\nModeration\nASSISTANTS\nOverview\nHow Assistants work\nTools\nGUIDES\nPrompt engineering\nProduction best practices\nSafety best practices\nRate limits\nError codes\nLibraries\nDeprecations\nPolicies\nCHATGPT\nActions\nPlugins\nText to speech\n\nLearn how to turn text into lifelike spoken audio\n\nIntroduction\n\nThe Audio API provides a speech endpoint based on our TTS (text-to-speech) model. It comes with 6 built-in voices and can be used to:\n\nNarrate a written blog post\nProduce spoken audio in multiple languages\nGive real time audio output using streaming\n\nHere is an example of the alloy voice:\n\n‍\nPlease note that our usage policies require you to provide a clear disclosure to end users that the TTS voice they are hearing is AI-generated and not a human voice.\nQuick start\n\nThe speech endpoint takes in three key inputs: the model, the text that should be turned into audio, and the voice to be used for the audio generation. A simple request would look like the following:\n\nGenerate spoken audio from input text\npython\nSelect library\npython\ncurl\nnode\nCopy‍\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\nfrom pathlib import Path\nfrom openai import OpenAI\nclient = OpenAI()\n\nspeech_file_path = Path(__file__).parent / \"speech.mp3\"\nresponse = client.audio.speech.create(\n  model=\"tts-1\",\n  voice=\"alloy\",\n  input=\"Today is a wonderful day to build something people love!\"\n)\n\nresponse.stream_to_file(speech_file_path)\n\nBy default, the endpoint will output a MP3 file of the spoken audio but it can also be configured to output any of our supported formats.\n\nAudio quality\n\nFor real-time applications, the standard tts-1 model provides the lowest latency but at a lower quality than the tts-1-hd model. Due to the way the audio is generated, tts-1 is likely to generate content that has more static in certain situations than tts-1-hd. In some cases, the audio may not have noticeable differences depending on your listening device and the individual person.\n\nVoice options\n\nExperiment with different voices (alloy, echo, fable, onyx, nova, and shimmer) to find one that matches your desired tone and audience. The current voices are optimized for English.\n\nAlloy\n‍\nEcho\n‍\nFable\n‍\nOnyx\n‍\nNova\n‍\nShimmer\n‍\nSupported output formats\n\nThe default response format is \"mp3\", but other formats like \"opus\", \"aac\", or \"flac\" are available.\n\nOpus: For internet streaming and communication, low latency.\nAAC: For digital audio compression, preferred by YouTube, Android, iOS.\nFLAC: For lossless audio compression, favored by audio enthusiasts for archiving.\nSupported languages\n\nThe TTS model generally follows the Whisper model in terms of language support. Whisper supports the following languages and performs well despite the current voices being optimized for English:\n\nAfrikaans, Arabic, Armenian, Azerbaijani, Belarusian, Bosnian, Bulgarian, Catalan, Chinese, Croatian, Czech, Danish, Dutch, English, Estonian, Finnish, French, Galician, German, Greek, Hebrew, Hindi, Hungarian, Icelandic, Indonesian, Italian, Japanese, Kannada, Kazakh, Korean, Latvian, Lithuanian, Macedonian, Malay, Marathi, Maori, Nepali, Norwegian, Persian, Polish, Portuguese, Romanian, Russian, Serbian, Slovak, Slovenian, Spanish, Swahili, Swedish, Tagalog, Tamil, Thai, Turkish, Ukrainian, Urdu, Vietnamese, and Welsh.\n\nYou can generate spoken audio in these languages by providing the input text in the language of your choice.\n\nStreaming real time audio\n\nThe Speech API provides support for real time audio streaming using chunk transfer encoding. This means that the audio is able to be played before the full file has been generated and made accessible.\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nresponse = client.audio.speech.create(\n    model=\"tts-1\",\n    voice=\"alloy\",\n    input=\"Hello world! This is a streaming test.\",\n)\n\nresponse.stream_to_file(\"output.mp3\")\nFAQ\nHow can I control the emotional range of the generated audio?\n\nThere is no direct mechanism to control the emotional output of the audio generated. Certain factors may influence the output audio like capitalization or grammar but our internal tests with these have yielded mixed results.\n\nCan I create a custom copy of my own voice?\n\nNo, this is not something we support.\n\nDo I own the outputted audio files?\n\nYes, like with all outputs from our API, the person who created them owns the output. You are still required to inform end users that they are hearing audio generated by AI and not a real person talking to them."
}