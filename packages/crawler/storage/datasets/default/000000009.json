{
	"title": "Embeddings - OpenAI API",
	"url": "https://platform.openai.com/docs/guides/embeddings",
	"html": "Overview\nDocumentation\nAPI reference\nExamples\nLog in\nSign up‍\nSearch\n⌘\nK\nGET STARTED\nIntroduction\nQuickstart\nModels\nTutorials\nChangelog\nCAPABILITIES\nText generation\nFunction calling\nEmbeddings\nOverview\nUse cases\nLimitations & risks\nFine-tuning\nImage generation\nVision\nText-to-speech\nSpeech-to-text\nModeration\nASSISTANTS\nOverview\nHow Assistants work\nTools\nGUIDES\nPrompt engineering\nProduction best practices\nSafety best practices\nRate limits\nError codes\nLibraries\nDeprecations\nPolicies\nCHATGPT\nActions\nPlugins\nEmbeddings\nWhat are embeddings?\n\nOpenAI’s text embeddings measure the relatedness of text strings. Embeddings are commonly used for:\n\nSearch (where results are ranked by relevance to a query string)\nClustering (where text strings are grouped by similarity)\nRecommendations (where items with related text strings are recommended)\nAnomaly detection (where outliers with little relatedness are identified)\nDiversity measurement (where similarity distributions are analyzed)\nClassification (where text strings are classified by their most similar label)\nWe are excited to announce that the new Assistants API comes with retrieval and built in message history management. If you don't want to worry about making and storing embeddings yourself, check out the Assistants API to learn more.\n\nAn embedding is a vector (list) of floating point numbers. The distance between two vectors measures their relatedness. Small distances suggest high relatedness and large distances suggest low relatedness.\n\nVisit our pricing page to learn about Embeddings pricing. Requests are billed based on the number of tokens in the input sent.\n\nTo see embeddings in action, check out our code samples\n\nClassification\nTopic clustering\nSearch\nRecommendations\nBrowse Samples‍\nHow to get embeddings\n\nTo get an embedding, send your text string to the embeddings API endpoint along with a choice of embedding model ID (e.g., text-embedding-ada-002). The response will contain an embedding, which you can extract, save, and use.\n\nExample requests:\n\nExample: Getting embeddings\ncurl\nSelect library\npython\ncurl\nCopy‍\n1\n2\n3\n4\n5\n6\n7\n\ncurl https://api.openai.com/v1/embeddings \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -d '{\n    \"input\": \"Your text string goes here\",\n    \"model\": \"text-embedding-ada-002\"\n  }'\n\nExample response:\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n{\n  \"data\": [\n    {\n      \"embedding\": [\n        -0.006929283495992422,\n        -0.005336422007530928,\n        ...\n        -4.547132266452536e-05,\n        -0.024047505110502243\n      ],\n      \"index\": 0,\n      \"object\": \"embedding\"\n    }\n  ],\n  \"model\": \"text-embedding-ada-002\",\n  \"object\": \"list\",\n  \"usage\": {\n    \"prompt_tokens\": 5,\n    \"total_tokens\": 5\n  }\n}\n\nSee more Python code examples in the OpenAI Cookbook.\n\nWhen using OpenAI embeddings, please keep in mind their limitations and risks.\n\nEmbedding models\n\nOpenAI offers one second-generation embedding model (denoted by -002 in the model ID) and 16 first-generation models (denoted by -001 in the model ID).\n\nWe recommend using text-embedding-ada-002 for nearly all use cases. It’s better, cheaper, and simpler to use. Read the blog post announcement.\n\nMODEL GENERATION\tTOKENIZER\tMAX INPUT TOKENS\tKNOWLEDGE CUTOFF\nV2\tcl100k_base\t8191\tSep 2021\nV1\tGPT-2/GPT-3\t2046\tAug 2020\n\nUsage is priced per input token, at a rate of $0.0004 per 1000 tokens, or about ~3,000 pages per US dollar (assuming ~800 tokens per page):\n\nMODEL\tROUGH PAGES PER DOLLAR\tEXAMPLE PERFORMANCE ON BEIR SEARCH EVAL\ntext-embedding-ada-002\t3000\t53.9\n*-davinci-*-001\t6\t52.8\n*-curie-*-001\t60\t50.9\n*-babbage-*-001\t240\t50.4\n*-ada-*-001\t300\t49.0\nSecond-generation models\nMODEL NAME\tTOKENIZER\tMAX INPUT TOKENS\tOUTPUT DIMENSIONS\ntext-embedding-ada-002\tcl100k_base\t8191\t1536\nFirst-generation models (not recommended)\nUse cases\n\nHere we show some representative use cases. We will use the Amazon fine-food reviews dataset for the following examples.\n\nObtaining the embeddings\n\nThe dataset contains a total of 568,454 food reviews Amazon users left up to October 2012. We will use a subset of 1,000 most recent reviews for illustration purposes. The reviews are in English and tend to be positive or negative. Each review has a ProductId, UserId, Score, review title (Summary) and review body (Text). For example:\n\nPRODUCT ID\tUSER ID\tSCORE\tSUMMARY\tTEXT\nB001E4KFG0\tA3SGXH7AUHU8GW\t5\tGood Quality Dog Food\tI have bought several of the Vitality canned...\nB00813GRG4\tA1D87F6ZCVE5NK\t1\tNot as Advertised\tProduct arrived labeled as Jumbo Salted Peanut...\n\nWe will combine the review summary and review text into a single combined text. The model will encode this combined text and output a single vector embedding.\n\nGet embeddings from dataset\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\nfrom openai import OpenAI\nclient = OpenAI()\n\ndef get_embedding(text, model=\"text-embedding-ada-002\"):\n   text = text.replace(\"\\n\", \" \")\n   return client.embeddings.create(input = [text], model=model).data[0].embedding\n\ndf['ada_embedding'] = df.combined.apply(lambda x: get_embedding(x, model='text-embedding-ada-002'))\ndf.to_csv('output/embedded_1k_reviews.csv', index=False)\n\nTo load the data from a saved file, you can run the following:\n\n1\n2\n3\n4\n\nimport pandas as pd\n\ndf = pd.read_csv('output/embedded_1k_reviews.csv')\ndf['ada_embedding'] = df.ada_embedding.apply(eval).apply(np.array)\nData visualization in 2D\nEmbedding as a text feature encoder for ML algorithms\nClassification using the embedding features\nZero-shot classification\nObtaining user and product embeddings for cold-start recommendation\nClustering\nText search using embeddings\nCode search using embeddings\nRecommendations using embeddings\nLimitations & risks\n\nOur embedding models may be unreliable or pose social risks in certain cases, and may cause harm in the absence of mitigations.\n\nSocial bias\n\nLimitation: The models encode social biases, e.g. via stereotypes or negative sentiment towards certain groups.\n\nWe found evidence of bias in our models via running the SEAT (May et al, 2019) and the Winogender (Rudinger et al, 2018) benchmarks. Together, these benchmarks consist of 7 tests that measure whether models contain implicit biases when applied to gendered names, regional names, and some stereotypes.\n\nFor example, we found that our models more strongly associate (a) European American names with positive sentiment, when compared to African American names, and (b) negative stereotypes with black women.\n\nThese benchmarks are limited in several ways: (a) they may not generalize to your particular use case, and (b) they only test for a very small slice of possible social bias.\n\nThese tests are preliminary, and we recommend running tests for your specific use cases. These results should be taken as evidence of the existence of the phenomenon, not a definitive characterization of it for your use case. Please see our usage policies for more details and guidance.\n\nPlease contact our support team via chat if you have any questions; we are happy to advise on this.\n\nBlindness to recent events\n\nLimitation: Models lack knowledge of events that occurred after August 2020.\n\nOur models are trained on datasets that contain some information about real world events up until 8/2020. If you rely on the models representing recent events, then they may not perform well.\n\nFrequently asked questions\nHow can I tell how many tokens a string has before I embed it?\n\nIn Python, you can split a string into tokens with OpenAI's tokenizer tiktoken.\n\nExample code:\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\nimport tiktoken\n\ndef num_tokens_from_string(string: str, encoding_name: str) -> int:\n    \"\"\"Returns the number of tokens in a text string.\"\"\"\n    encoding = tiktoken.get_encoding(encoding_name)\n    num_tokens = len(encoding.encode(string))\n    return num_tokens\n\nnum_tokens_from_string(\"tiktoken is great!\", \"cl100k_base\")\n\nFor second-generation embedding models like text-embedding-ada-002, use the cl100k_base encoding.\n\nMore details and example code are in the OpenAI Cookbook guide how to count tokens with tiktoken.\n\nHow can I retrieve K nearest embedding vectors quickly?\n\nFor searching over many vectors quickly, we recommend using a vector database. You can find examples of working with vector databases and the OpenAI API in our Cookbook on GitHub.\n\nVector database options include:\n\nChroma, an open-source embeddings store\nElasticsearch, a popular search/analytics engine and vector database\nMilvus, a vector database built for scalable similarity search\nPinecone, a fully managed vector database\nQdrant, a vector search engine\nRedis as a vector database\nTypesense, fast open source vector search\nWeaviate, an open-source vector search engine\nZilliz, data infrastructure, powered by Milvus\nWhich distance function should I use?\n\nWe recommend cosine similarity. The choice of distance function typically doesn’t matter much.\n\nOpenAI embeddings are normalized to length 1, which means that:\n\nCosine similarity can be computed slightly faster using just a dot product\nCosine similarity and Euclidean distance will result in the identical rankings\nCan I share my embeddings online?\n\nCustomers own their input and output from our models, including in the case of embeddings. You are responsible for ensuring that the content you input to our API does not violate any applicable law or our Terms of Use."
}