{
	"title": "Image generation - OpenAI API",
	"url": "https://platform.openai.com/docs/guides/images/introduction?context=node",
	"html": "Overview\nDocumentation\nAPI reference\nExamples\nLog in\nSign up‍\nSearch\n⌘\nK\nGET STARTED\nIntroduction\nQuickstart\nModels\nTutorials\nChangelog\nCAPABILITIES\nText generation\nFunction calling\nEmbeddings\nFine-tuning\nImage generation\nIntroduction\nUsage\nLanguage-specific tips\nVision\nText-to-speech\nSpeech-to-text\nModeration\nASSISTANTS\nOverview\nHow Assistants work\nTools\nGUIDES\nPrompt engineering\nProduction best practices\nSafety best practices\nRate limits\nError codes\nLibraries\nDeprecations\nPolicies\nCHATGPT\nActions\nPlugins\nImage generation\n\nLearn how to generate or manipulate images with DALL·E in the API.\n\nLooking to generate images in ChatGPT? Head to chat.openai.com.\nIntroduction\n\nThe Images API provides three methods for interacting with images:\n\nCreating images from scratch based on a text prompt (DALL·E 3 and DALL·E 2)\nCreating edited versions of images by having the model replace some areas of a pre-existing image, based on a new text prompt (DALL·E 2 only)\nCreating variations of an existing image (DALL·E 2 only)\n\nThis guide covers the basics of using these three API endpoints with useful code samples. To try DALL·E 3, head to ChatGPT. To try DALL·E 2, check out the DALL·E preview app.\n\nUsage\nGenerations\n\nThe image generations endpoint allows you to create an original image given a text prompt. When using DALL·E 3, images can have a size of 1024x1024, 1024x1792 or 1792x1024 pixels.\n\nBy default, images are generated at standard quality, but when using DALL·E 3 you can set quality: \"hd\" for enhanced detail. Square, standard quality images are the fastest to generate.\n\nYou can request 1 image at a time with DALL·E 3 (request more by making parallel requests) or up to 10 images at a time using DALL·E 2 with the n parameter.\n\nGenerate an image\npython\nSelect library\npython\nnode.js\ncurl\nCopy‍\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\nfrom openai import OpenAI\nclient = OpenAI()\n\nresponse = client.images.generate(\n  model=\"dall-e-3\",\n  prompt=\"a white siamese cat\",\n  size=\"1024x1024\",\n  quality=\"standard\",\n  n=1,\n)\n\nimage_url = response.data[0].url\nWhat is new with DALL·E 3\nExplore what is new with DALL·E 3 in the OpenAI Cookbook\nPrompting\n\nWith the release of DALL·E 3, the model now takes in the default prompt provided and automatically re-write it for safety reasons, and to add more detail (more detailed prompts generally result in higher quality images).\n\nWhile it is not currently possible to disable this feature, you can use prompting to get outputs closer to your requested image by adding the following to your prompt: I NEED to test how the tool works with extremely simple prompts. DO NOT add any detail, just use it AS-IS:.\n\nThe updated prompt is visible in the revised_prompt field of the data response object.\n\nExample DALL·E 3 generations\nPROMPT\tGENERATION\nA photograph of a white Siamese cat.\t\n\nEach image can be returned as either a URL or Base64 data, using the response_format parameter. URLs will expire after an hour.\n\nEdits (DALL·E 2 only)\n\nAlso known as \"inpainting\", the image edits endpoint allows you to edit or extend an image by uploading an image and mask indicating which areas should be replaced. The transparent areas of the mask indicate where the image should be edited, and the prompt should describe the full new image, not just the erased area. This endpoint can enable experiences like the editor in our DALL·E preview app.\n\nEdit an image\npython\nSelect library\npython\nnode.js\ncurl\nCopy‍\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\nfrom openai import OpenAI\nclient = OpenAI()\n\nresponse = client.images.edit((\n  model=\"dall-e-2\",\n  image=open(\"sunlit_lounge.png\", \"rb\"),\n  mask=open(\"mask.png\", \"rb\"),\n  prompt=\"A sunlit indoor lounge area with a pool containing a flamingo\",\n  n=1,\n  size=\"1024x1024\"\n)\nimage_url = response.data[0].url\nIMAGE\tMASK\tOUTPUT\n\t\t\n\nPrompt: a sunlit indoor lounge area with a pool containing a flamingo\n\nThe uploaded image and mask must both be square PNG images less than 4MB in size, and also must have the same dimensions as each other. The non-transparent areas of the mask are not used when generating the output, so they don’t necessarily need to match the original image like the example above.\n\nVariations (DALL·E 2 only)\n\nThe image variations endpoint allows you to generate a variation of a given image.\n\nGenerate an image variation\npython\nSelect library\npython\nnode.js\ncurl\nCopy‍\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\nfrom openai import OpenAI\nclient = OpenAI()\n\nresponse = client.images.create_variation(\n  image=open(\"image_edit_original.png\", \"rb\"),\n  n=2,\n  size=\"1024x1024\"\n)\n\nimage_url = response.data[0].url\nIMAGE\tOUTPUT\n\t\n\nSimilar to the edits endpoint, the input image must be a square PNG image less than 4MB in size.\n\nContent moderation\n\nPrompts and images are filtered based on our content policy, returning an error when a prompt or image is flagged.\n\nLanguage-specific tips\nNode.js‍\nPython‍\nUsing in-memory image data\n\nThe Node.js examples in the guide above use the fs module to read image data from disk. In some cases, you may have your image data in memory instead. Here's an example API call that uses image data stored in a Node.js Buffer object:\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\nimport OpenAI from \"openai\";\n\nconst openai = new OpenAI();\n\n// This is the Buffer object that contains your image data\nconst buffer = [your image data];\n\n// Set a `name` that ends with .png so that the API knows it's a PNG image\nbuffer.name = \"image.png\";\n\nasync function main() {\n  const image = await openai.images.createVariation({ model: \"dall-e-2\", image: buffer, n: 1, size: \"1024x1024\" });\n  console.log(image.data);\n}\nmain();\nWorking with TypeScript\n\nIf you're using TypeScript, you may encounter some quirks with image file arguments. Here's an example of working around the type mismatch by explicitly casting the argument:\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\nimport fs from \"fs\";\nimport OpenAI from \"openai\";\n\nconst openai = new OpenAI();\n\nasync function main() {\n  // Cast the ReadStream to `any` to appease the TypeScript compiler\n  const image = await openai.images.createVariation({\n    image: fs.createReadStream(\"image.png\") as any,\n  });\n\n  console.log(image.data);\n}\nmain();\n\nAnd here's a similar example for in-memory image data:\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\nimport fs from \"fs\";\nimport OpenAI from \"openai\";\n\nconst openai = new OpenAI();\n\n// This is the Buffer object that contains your image data\nconst buffer: Buffer = [your image data];\n\n// Cast the buffer to `any` so that we can set the `name` property\nconst file: any = buffer;\n\n// Set a `name` that ends with .png so that the API knows it's a PNG image\nfile.name = \"image.png\";\n\nasync function main() {\n  const image = await openai.images.createVariation({\n    file,\n    1,\n    \"1024x1024\"\n  });\n  console.log(image.data);\n}\nmain();\nError handling\n\nAPI requests can potentially return errors due to invalid inputs, rate limits, or other issues. These errors can be handled with a try...catch statement, and the error details can be found in either error.response or error.message:\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\nimport fs from \"fs\";\nimport OpenAI from \"openai\";\n\nconst openai = new OpenAI();\n\ntry {\n    const response = openai.images.createVariation(\n        fs.createReadStream(\"image.png\"),\n        1,\n        \"1024x1024\"\n    );\n    console.log(response.data.data[0].url);\n} catch (error) {\n    if (error.response) {\n        console.log(error.response.status);\n        console.log(error.response.data);\n    } else {\n        console.log(error.message);\n    }\n}"
}